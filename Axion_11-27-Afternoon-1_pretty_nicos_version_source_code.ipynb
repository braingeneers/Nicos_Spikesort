{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\" size=7>Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the source code that runs the `spikesort_easy` application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">This code is based on [this repository](https://github.com/flatironinstitute/spikeforest2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/braingeneers/dashboard ./dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dashboard.apps.utils import NumpyS3Memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tenacity\n",
    "#!pip install smart_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import kachery as ka\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "#do things to get braingeneers data\n",
    "#install(\"git+https://github.com/braingeneers/braingeneerspy.git\")\n",
    "#install(\"matplotlib\")\n",
    "import hither_sf as hither\n",
    "from spikeforest2 import sorters\n",
    "import matplotlib\n",
    "import braingeneers.datasets_electrophysiology\n",
    "import spikeinterface.extractors as se #for mda conversion only\n",
    "import spiketoolkit as st\n",
    "import braingeneers\n",
    "from braingeneers import datasets_electrophysiology as ephys\n",
    "import numpy as np\n",
    "import json\n",
    "import spikeforest2_utils\n",
    "from spikeforest2_utils import AutoRecordingExtractor, MdaRecordingExtractor\n",
    "import hither_sf as hither\n",
    "import kachery as ka\n",
    "import io\n",
    "import sys\n",
    "spikeforest2_utils.__version__=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folders = !ls\n",
    "if \"kach_dir\" not in folders:\n",
    "    !mkdir kach_dir\n",
    "    !touch kach_dir/raw.mda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current_dir = ! pwd\n",
    "kach_dir = current_dir[0] + \"/kach_dir/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Choose Experiment, time range, channels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">choose time range, experiment name, experiment num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.environ['KACHERY_STORAGE_DIR']=kach_dir\n",
    "#batch_uuid = '2020-11-27-e-UCSF-axionplate' \n",
    "#experiment_num= 9 #arpitha make this a button to choose exp num\n",
    "#start = 1 #arpitha, sliders here for start minutes\n",
    "#stop = 5 #and end minutes\n",
    "#recording_path = 'sha1://961f4a641af64dded4821610189f808f0192de4d/SYNTH_MEAREC_TETRODE/synth_mearec_tetrode_noise10_K10_C4/002_synth.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --endpoint https://s3.nautilus.optiputer.net s3 ls s3://braingeneersdev/ephys/2020-11-27-e-primary-axion-Afternoon1/numpy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws --endpoint https://s3.nautilus.optiputer.net s3 ls s3://braingeneersdev/ephys/2020-11-27-e-primary-axion-Afternoon1/numpy/well_A1_chan_group_idx_3_time_000.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_numpy = \"s3://braingeneersdev/ephys/2020-11-27-e-primary-axion-Afternoon1/numpy/well_A2_chan_group_idx_10_time_001.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export ENDPOINT_URL=https://s3.nautilus.optiputer.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ENDPOINT_URL'] = \"https://s3.nautilus.optiputer.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import functools\n",
    "from smart_open import open\n",
    "open = functools.partial(open, transport_params={'resource_kwargs':{'endpoint_url': os.environ['ENDPOINT_URL']}})  # wraps open setting a default endpoint_url\n",
    "#data = NumpyS3Memmap('s3://braingeneersdev/dfparks/test/test.npy')\n",
    "data = NumpyS3Memmap(this_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=10):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, frame.filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using open now works with the PRP exactly the same way it does if the file is local\n",
    "#with open('s3://braingeneersdev/dfparks/test/hello.txt', 'r') as f:\n",
    "#with open(this_numpy, 'r') as f:\n",
    "#    print(\"sh\")\n",
    "#Hello World!\n",
    "# Example using open with a local file, identical to the S3 case\n",
    "#with open('/tmp/hello.txt', 'r') as f:\n",
    "#    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from apps.utils import NumpyS3Memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = tracemalloc.take_snapshot()\n",
    "display_top(snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## view other experiment numbers in the same batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data, time array (seconds), and sampling freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, t, fs = ephys.load_blocks(batch_uuid, experiment_num, start=start, stop=stop)\n",
    "X = data\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 22000 #????\n",
    "#X = X.T #spikeinterface wants it MxN (channel number by time)\n",
    "X.shape\n",
    "\n",
    "Y = np.append(X[:,0:1], X[:,1:2], axis=1)\n",
    "#Z = np.append(X[:,2:3], X[:,3:4], axis=1)\n",
    "Y = Y.T\n",
    "\n",
    "#M = np.append(Y,Z,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"this is sampling Freq: \", fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">optionally (ideally): apply geometry for locations of electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_chan = X[selected_channels[0],:]\n",
    "# print(\"last_chan shape \", last_chan.shape)\n",
    "# print(\"X[0].shape \", X[0].shape)\n",
    "\n",
    "\n",
    "#geom = np.zeros((X.shape[1],2))\n",
    "#geom = np.zeros((1,2)) # initialize recording device geometry to zeros\n",
    "\n",
    "# if len(selected_channels)==1:\n",
    "#     this_chan = X[selected_channels[0]]\n",
    "#     this_chan = np.array([this_chan])\n",
    "#     X = this_chan\n",
    "#     print(\"1 chan: \", X.shape)\n",
    "    \n",
    "    \n",
    "# if len(selected_channels)==2:\n",
    "#     X = np.append([X[selected_channels[0]]], [X[selected_channels[1]]], axis=0)\n",
    "#     print(\"2 chan: \", X.shape)\n",
    "#     geom = np.array([[0.0,0.0],[0.0,1.0]])\n",
    "\n",
    "# if len(selected_channels)>2: #PROBLEMATIC\n",
    "#     for chan in range(1,len(selected_channels)):\n",
    "#         last_chan = np.append([last_chan],[X[selected_channels[chan]]],axis=0)\n",
    "#     X = last_chan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.shape\n",
    "#X[1:10,1:10]\n",
    "print(\"minutes = \", X.shape[0]/fs/60)\n",
    "#print(\"expected minutes =\", stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X.shape[0] == 2 :\n",
    "    geom = np.array([[0.0,0.0],[0.0,1.0]])\n",
    "    print(\"only two channels, so Geom always two points 1 unit apart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geom.shape #the 2d locations in space of each channel on the recording device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create \"Recording\" for spikeForest and Filter recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording=se.NumpyRecordingExtractor(\n",
    "    timeseries=X,\n",
    "    geom=geom,\n",
    "    sampling_frequency=fs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recording = st.preprocessing.bandpass_filter(recording, freq_min=300, freq_max=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_path = ka.store_file(fname  + 'raw.mda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kach_dir_files = ! ls kach_dir\n",
    "if \"geom.csv\" not in kach_dir_files :\n",
    "    ! touch kach_dir/geom.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obj = dict(\n",
    "    raw=raw_path,\n",
    "    params=ka.load_object(fname + 'params.json'),\n",
    "    geom=np.genfromtxt(ka.load_file(fname + 'geom.csv'), delimiter=',').tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obj['self_reference'] = ka.store_object(obj, basename='{}.json'.format(label))\n",
    "print(obj['self_reference'])\n",
    "with open(output_fname, 'w') as f:\n",
    "    json.dump(obj, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_recording(*, fname, recording, output_fname, label):\n",
    "            raw_path = ka.store_file(fname  + 'raw.mda')\n",
    "            obj = dict(\n",
    "                raw=raw_path,\n",
    "                params=ka.load_object(fname + 'params.json'),\n",
    "                #geom=np.genfromtxt(ka.load_file(fname + 'geom.csv'), delimiter=',').tolist()\n",
    "            )\n",
    "            obj['self_reference'] = ka.store_object(obj, basename='{}.json'.format(label))\n",
    "\n",
    "            with open(output_fname, 'w') as f: json.dump(obj, f, indent=4)\n",
    "            return obj['self_reference']\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_path =  register_recording( recording=recording, fname=fname, output_fname=fname+'new_recording.json', label='new_recording')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"black\"> Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy dataset courtesy of spikeforest's example \n",
    "#recording_path = 'sha1://961f4a641af64dded4821610189f808f0192de4d/SYNTH_MEAREC_TETRODE/synth_mearec_tetrode_noise10_K10_C4/002_synth.json' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ka.config(fr='default_readonly'):\n",
    "    #with hither.config(cache='default_readwrite'):\n",
    "        with hither.config(container='default'):\n",
    "            result = sorters.spykingcircus.run(\n",
    "                    recording_path=recording_path,\n",
    "                    sorting_out=hither.File()\n",
    "                    )\n",
    "            print(\"recording out path: \", recording_path)\n",
    "            print(\"sorting out path: \",result.outputs.sorting_out)\n",
    "            sorting_path = str(result.outputs.sorting_out)\n",
    "\n",
    "print(result.outputs.sorting_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_out_short = sorting_path[12:-1]\n",
    "\n",
    "print(\"This is the new short path for the sorting: \",sorting_out_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeforest2_utils import AutoRecordingExtractor, AutoSortingExtractor\n",
    "import kachery as ka\n",
    "\n",
    "universal_sorting_path = ka.store_file(sorting_out_short)\n",
    "\n",
    "recording = AutoRecordingExtractor(recording_path, download=False)\n",
    "sorting_true = AutoSortingExtractor(universal_sorting_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can access the recording using the SpikeInterface API\n",
    "# For example, print the electrode locations and unit IDs\n",
    "print(recording.get_channel_locations())\n",
    "print(sorting_true.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.widgets as sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts = sw.plot_timeseries(recording)\n",
    "    \n",
    "    #does not work w no geom:\n",
    "    #w_el = sw.plot_electrode_geometry(recording)\n",
    "    #pickle.dump(w_el, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "w_sp = sw.plot_spectrum(recording)\n",
    "    \n",
    "w_rs = sw.plot_rasters(sorting_true, sampling_frequency=fs)\n",
    "    \n",
    "w_isi = sw.plot_isi_distribution(sorting_true, sampling_frequency=fs, bins=10, window=1)\n",
    "    \n",
    "w_ach = sw.plot_autocorrelograms(sorting_true, sampling_frequency=fs, bin_size=1, window=10, unit_ids=[1, 2, 4, 5, 8, 10, 7])\n",
    "    \n",
    "w_cch = sw.plot_crosscorrelograms(sorting_true, sampling_frequency=fs, unit_ids=[1, 5, 8], bin_size=0.1, window=5)\n",
    "\n",
    "w_wf = sw.plot_unit_waveforms(recording, sorting_true,unit_ids=sorting_true.get_unit_ids(),  max_spikes_per_unit=100)\n",
    "    \n",
    "w_ampd = sw.plot_amplitudes_distribution(recording, sorting_true, max_spikes_per_unit=300)\n",
    "    \n",
    "w_ampt = sw.plot_amplitudes_timeseries(recording, sorting_true, max_spikes_per_unit=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Curate Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "curated_units = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w_wf = sw.plot_unit_waveforms(recording, sorting_true,unit_ids=curated_units,max_channels=1,color='darkgreen', max_spikes_per_unit=300)#,figure=fig)#,axis=)\n",
    "\n",
    "plt.yticks(np.arange(-1.5, 2.0, 1))\n",
    "print(\"Each graph spans 3ms x axis. Y axis is microvolts -- the same as the amplitude distributions in the other graphs\")\n",
    "w_wf = sw.plot_unit_waveforms(recording, sorting_true,unit_ids=[curated_units[1],curated_units[0]],max_channels=1,color='darkgreen', max_spikes_per_unit=300)#,figure=fig)#,axis=)\n",
    "\n",
    "plt.yticks(np.arange(-1.5, 2.0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker ps -a | grep Exit | cut -d ' ' -f 1 | xargs sudo docker rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import os\n",
    "import tracemalloc\n",
    "\n",
    "def display_top(snapshot, key_type='lineno', limit=10):\n",
    "    snapshot = snapshot.filter_traces((\n",
    "        tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\n",
    "        tracemalloc.Filter(False, \"<unknown>\"),\n",
    "    ))\n",
    "    top_stats = snapshot.statistics(key_type)\n",
    "\n",
    "    print(\"Top %s lines\" % limit)\n",
    "    for index, stat in enumerate(top_stats[:limit], 1):\n",
    "        frame = stat.traceback[0]\n",
    "        print(\"#%s: %s:%s: %.1f KiB\"\n",
    "              % (index, frame.filename, frame.lineno, stat.size / 1024))\n",
    "        line = linecache.getline(frame.filename, frame.lineno).strip()\n",
    "        if line:\n",
    "            print('    %s' % line)\n",
    "\n",
    "    other = top_stats[limit:]\n",
    "    if other:\n",
    "        size = sum(stat.size for stat in other)\n",
    "        print(\"%s other: %.1f KiB\" % (len(other), size / 1024))\n",
    "    total = sum(stat.size for stat in top_stats)\n",
    "    print(\"Total allocated size: %.1f KiB\" % (total / 1024))\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
